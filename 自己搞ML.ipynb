{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round]: 1, [MODEL]: 1, [EPOCH]: 999, [LOSS]: 5.562027\n"
     ]
    }
   ],
   "source": [
    "'''0907避免kernel dead'''\n",
    "'''ML_noerror.csv'''\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "# N = 1000  # num_samples_per_class\n",
    "D = 5  # dimensions\n",
    "C = 1  # num_classes\n",
    "H = 100  # num_hidden_units\n",
    "\n",
    "filename = \"ML_noerror.csv\"     # print(data)\n",
    "data = np.genfromtxt(filename, delimiter = ',', dtype = None, skip_header = 0, encoding = 'UTF-8')     #, usecols = [7,15,37,38,53]\n",
    "[Row,Column] = data.shape     # 確認data(?)...print(data.shape); print(data); print(type(data[1,0]))\n",
    "learning_rate = 2e-2\n",
    "lambda_l2 = 9e-5\n",
    "n_networks = 1     # Number of networks\n",
    "criterion = torch.nn.MSELoss()     # nn package also has different loss functions. We use MSE for a regression task\n",
    "totalerr_train = [];\n",
    "totalerr_test = [];\n",
    "total_corr = [];\n",
    "\n",
    "for i in range(10):\n",
    "    train_set = [];\n",
    "    test_set = [];\n",
    "    models = list()\n",
    "    y_pretrain = list()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for j in range(len(data)):\n",
    "        if j%10 == i:\n",
    "            test_set.append(j)\n",
    "        else:\n",
    "            train_set.append(j)\n",
    "    X = torch.from_numpy(data[train_set,1:6].astype(np.float64))\n",
    "    y = torch.from_numpy(data[train_set,0:1].astype(np.float64))\n",
    "    X_test = torch.from_numpy(data[test_set,1:6].astype(np.float64))\n",
    "    y_test = torch.from_numpy(data[test_set,0:1].astype(np.float64))\n",
    "    for mod in range(n_networks):\n",
    "      # nn package to create our linear model     # https://stackoverflow.com/questions/46141690/how-do-i-write-a-pytorch-sequential-model\n",
    "      # each Linear module has a weight and bias     # https://discuss.pytorch.org/t/mlp-with-one-hidden-layer-dimensions-modules/48841\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(D, H),\n",
    "            nn.ReLU(),     # if mod < n_networks // 2 else nn.Tanh(),\n",
    "            nn.Linear(H, C)\n",
    "        )\n",
    "        model.to(device)\n",
    "      # Append models\n",
    "        models.append(model)\n",
    "      # we use the optim package to apply\n",
    "      # ADAM for our parameter updates\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2)     # built-in L2\n",
    "      # e = 1.  # plotting purpose.\n",
    "      ## Training\n",
    "        for t in range(1000):\n",
    "          # Feed forward to get the logits     # print(\"test_set = \", test_set);     # 檢查是否正確取樣\n",
    "            y_pred = model(X.float())\n",
    "          # Append pre-train output\n",
    "            if t == 0:\n",
    "                y_pretrain.append(y_pred.detach())\n",
    "          # Compute the loss and accuracy\n",
    "            loss = torch.sqrt(criterion(y_pred.float(), y.float()))\n",
    "            print(f\"[Round]: {i+1}, [MODEL]: {mod + 1}, [EPOCH]: {t}, [LOSS]: {loss.item():.6f}\")\n",
    "            display.clear_output(wait=True)\n",
    "          # zero the gradients before running\n",
    "          # the backward pass.\n",
    "            optimizer.zero_grad()\n",
    "          # Backward pass to compute the gradient\n",
    "          # of loss w.r.t our learnable params. \n",
    "            loss.backward()\n",
    "          # Update params\n",
    "            optimizer.step()\n",
    "            if t == 999:\n",
    "                totalerr_train.append(np.round(loss.item(),6))  \n",
    "              ## Testing\n",
    "                y_test_pred = model(X_test.float())\n",
    "              # Compute the loss and accuracy\n",
    "                loss_test = torch.sqrt(criterion(y_test_pred.float(), y_test.float()))\n",
    "                y_test_array = []; y_test_pred_array = [];\n",
    "                for i in range(len(y_test)):\n",
    "                    y_test_array.append(y_test.detach().numpy()[i][0].astype(np.float64))\n",
    "                    y_test_pred_array.append(y_test_pred.detach().numpy()[i][0].astype(np.float64))\n",
    "                R = np.corrcoef(y_test_array, y_test_pred_array)[0,1];     ##[0,1]\n",
    "                R_sq = np.round(R*R,3)\n",
    "                totalerr_test.append(np.round(loss_test.item(),6))\n",
    "                total_corr.append(np.round(R_sq,3))\n",
    "\n",
    "print(\"Result_train = \", totalerr_train); print(np.round(np.mean(totalerr_train),3))\n",
    "print(\"Result_test = \", totalerr_test); print(np.round(np.mean(totalerr_test),3))\n",
    "print(\"total_corr(^2) = \", total_corr); print(np.round(np.mean(total_corr),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeced197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3300e-01,  1.1700e+03,  8.0069e+00,  1.5000e+03,  8.7500e+00],\n",
      "        [-2.1400e-01,  1.2300e+03,  7.9929e+00,  1.7100e+03,  8.6700e+00],\n",
      "        [-2.3300e-01,  7.9600e+02,  8.0034e+00,  1.5700e+03,  8.7300e+00]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[100.],\n",
      "        [100.],\n",
      "        [100.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "'''Input資料'''\n",
    "filename = \"ML_noerror.csv\";     # print(data) usecols = range(2,8,1),\n",
    "data = np.genfromtxt(filename, delimiter = ',', dtype = None, skip_header = 0, encoding = 'UTF-8');     #, usecols = [7,15,37,38,53]\n",
    "[Row,Column] = data.shape;     # 確認data(?)...print(data.shape); print(data); print(type(data[1,0]))\n",
    "X = torch.from_numpy(data[:,1:6].astype(np.float64));     # features\n",
    "y = torch.from_numpy(data[:,0:1].astype(np.float64));     # SOH_dis (放電Ah / 35min(Ah))\n",
    "print(X[:3])\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d086d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99993704]\n",
      " [0.9999225 ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "random.seed(1)\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "for iteration in range(10000):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))\n",
    "print(1 / (1 + exp(-(dot(array([[1, 0, 0],[1, 1, 0]]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae2ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06791109327547613\n",
      "0.7383781203733911\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''Input資料'''\n",
    "X, Y = sklearn.datasets.make_moons(n_samples=500, noise=.2)\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def setParameters(X, Y, hidden_size):\n",
    "    np.random.seed(3)\n",
    "    input_size = X.shape[0] # number of neurons in input layer\n",
    "    output_size = Y.shape[0] # number of neurons in output layer.\n",
    "    W1 = np.random.randn(hidden_size, input_size)*np.sqrt(2/input_size)\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size)*np.sqrt(2/hidden_size)\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "# Python implementation\n",
    "    np.random.randn(output_size, hidden_size)*np.sqrt(2/hidden_size)\n",
    "    np.random.randn(output_size, hidden_size)*0.01\n",
    "\n",
    "def forwardPropagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    y = sigmoid(Z2)  \n",
    "    return y, {'Z1': Z1, 'Z2': Z2, 'A1': A1, 'y': y}\n",
    "\n",
    "def cost(predict, actual):\n",
    "    m = actual.shape[1]\n",
    "    cost__ = -np.sum(np.multiply(np.log(predict), actual) + np.multiply((1 - actual), np.log(1 - predict)))/m\n",
    "    return np.squeeze(cost__)\n",
    "\n",
    "def backPropagation(X, Y, params, cache):\n",
    "    m = X.shape[1]\n",
    "    dy = cache['y'] - Y\n",
    "    dW2 = (1 / m) * np.dot(dy, np.transpose(cache['A1']))\n",
    "    db2 = (1 / m) * np.sum(dy, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(np.transpose(params['W2']), dy) * (1-np.power(cache['A1'], 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, np.transpose(X))\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def updateParameters(gradients, params, learning_rate = 1.2):\n",
    "    W1 = params['W1'] - learning_rate * gradients['dW1']\n",
    "    b1 = params['b1'] - learning_rate * gradients['db1']\n",
    "    W2 = params['W2'] - learning_rate * gradients['dW2']\n",
    "    b2 = params['b2'] - learning_rate * gradients['db2']\n",
    "    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "\n",
    "def fit(X, Y, learning_rate, hidden_size, number_of_iterations = 5000):\n",
    "    params = setParameters(X, Y, hidden_size)\n",
    "    cost_ = []\n",
    "    for j in range(number_of_iterations):\n",
    "        y, cache = forwardPropagation(X, params)\n",
    "        costit = cost(y, Y)\n",
    "        gradients = backPropagation(X, Y, params, cache)\n",
    "        params = updateParameters(gradients, params, learning_rate)\n",
    "        cost_.append(costit)\n",
    "    return params, cost_\n",
    "\n",
    "params, cost_ = fit(X, Y, 0.3, 5, 5000)\n",
    "plt.plot(cost_)\n",
    "print(min(cost_))\n",
    "print(max(cost_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "class dlnet:\n",
    "    def __init__(self, x, y):\n",
    "        self.X=x\n",
    "        self.Y=y\n",
    "        self.Yh=np.zeros((1,self.Y.shape[1]))\n",
    "        self.L=2\n",
    "        self.dims = [9, 15, 1]\n",
    "        self.param = {}\n",
    "        self.ch = {}\n",
    "        self.grad = {}\n",
    "        self.loss = []\n",
    "        self.lr=0.003\n",
    "        self.sam = self.Y.shape[1]\n",
    "        \n",
    "def nInit(self):\n",
    "    np.random.seed(1)\n",
    "    self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
    "    self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
    "    self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
    "    self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
    "    return\n",
    "\n",
    "def Sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def Relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def forward(self):\n",
    "    Z1 = self.param['W1'].dot(self.X) + self.param['b1'] \n",
    "    A1 = Relu(Z1)\n",
    "    self.ch['Z1'],self.ch['A1']=Z1,A1\n",
    "        \n",
    "    Z2 = self.param['W2'].dot(A1) + self.param['b2']  \n",
    "    A2 = Sigmoid(Z2)\n",
    "    self.ch['Z2'],self.ch['A2']=Z2,A2\n",
    "    self.Yh=A2\n",
    "    loss=self.nloss(A2)\n",
    "    return self.Yh, loss\n",
    "    \n",
    "squared_errors = (self.Yh - self.Y) ** 2\n",
    "self.Loss= np.sum(squared_errors)\n",
    "def nloss(self,Yh):\n",
    "    loss = (1./self.sam) * (-np.dot(self.Y,np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))    \n",
    "    return loss\n",
    "    \n",
    "def dRelu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "def dSigmoid(Z):\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = s * (1-s)\n",
    "    return dZ\n",
    "def backward(self):\n",
    "    dLoss_Yh = - (np.divide(self.Y, self.Yh ) - np.divide(1 - self.Y, 1 - self.Yh))    \n",
    "        \n",
    "    dLoss_Z2 = dLoss_Yh * dSigmoid(self.ch['Z2'])    \n",
    "    dLoss_A1 = np.dot(self.param[\"W2\"].T,dLoss_Z2)\n",
    "    dLoss_W2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2,self.ch['A1'].T)\n",
    "    dLoss_b2 = 1./self.ch['A1'].shape[1] * np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1])) \n",
    "                            \n",
    "    dLoss_Z1 = dLoss_A1 * dRelu(self.ch['Z1'])        \n",
    "    dLoss_A0 = np.dot(self.param[\"W1\"].T,dLoss_Z1)\n",
    "    dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)\n",
    "    dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))  \n",
    "        \n",
    "    self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1\n",
    "    self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1\n",
    "    self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2\n",
    "    self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2\n",
    "        \n",
    "nn = dlnet(x,y)\n",
    "nn.gd(x, y, iter = 15000)\n",
    "def gd(self,X, Y, iter = 3000):\n",
    "    np.random.seed(1)                         \n",
    "    self.nInit()\n",
    "    for i in range(0, iter):\n",
    "        Yh, loss=self.forward()\n",
    "        self.backward()\n",
    "        if i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "            self.loss.append(loss)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f51ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d74e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1) Random starting synaptic weights: \n",
      "    Layer 1 (4 neurons, each with 3 inputs): \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n",
      "Stage 2) New synaptic weights after training: \n",
      "    Layer 1 (4 neurons, each with 3 inputs): \n",
      "[[ 0.3122465   4.57704063 -6.15329916 -8.75834924]\n",
      " [ 0.19676933 -8.74975548 -6.1638187   4.40720501]\n",
      " [-0.03327074 -0.58272995  0.08319184 -0.39787635]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[ -8.18850925]\n",
      " [ 10.13210706]\n",
      " [-21.33532796]\n",
      " [  9.90935111]]\n",
      "Stage 3) Considering a new situation [1, 1, 0] -> ?: \n",
      "[[0.0078876  0.99481481]]\n"
     ]
    }
   ],
   "source": [
    "'''example for XOR problem'''\n",
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class NeuronLayer():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
    "        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "\n",
    "  # The Sigmoid function, pass the weighted sum of the inputs through this function to\n",
    "  # normalize them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "  # The derivative of the Sigmoid function,indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "  # We train the neural network through a process of trial and error.\n",
    "  # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "          # Pass the training set through our neural network\n",
    "            output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\n",
    "\n",
    "          # Calculate the error for layer 2 (The difference between the desired output\n",
    "          # and the predicted output).\n",
    "            layer2_error = training_set_outputs - output_from_layer_2\n",
    "            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n",
    "\n",
    "          # Calculate the error for layer 1 (By looking at the weights in layer 1,\n",
    "          # we can determine by how much layer 1 contributed to the error in layer 2).\n",
    "            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n",
    "            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
    "\n",
    "          # Calculate how much to adjust the weights by\n",
    "            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n",
    "            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n",
    "\n",
    "          # Adjust the weights.\n",
    "            self.layer1.synaptic_weights += layer1_adjustment\n",
    "            self.layer2.synaptic_weights += layer2_adjustment\n",
    "\n",
    "  # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "\n",
    "  # The neural network prints its weights\n",
    "    def print_weights(self):\n",
    "        print(\"    Layer 1 (4 neurons, each with 3 inputs): \") \n",
    "        print(self.layer1.synaptic_weights) \n",
    "        print(\"    Layer 2 (1 neuron, with 4 inputs):\") \n",
    "        print(self.layer2.synaptic_weights) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  # Seed the random number generator\n",
    "    random.seed(1)\n",
    "\n",
    "  # Create layer 1 (4 neurons, each with 3 inputs)\n",
    "    layer1 = NeuronLayer(4, 3)\n",
    "\n",
    "  # Create layer 2 (a single neuron with 4 inputs)\n",
    "    layer2 = NeuronLayer(1, 4)\n",
    "\n",
    "  # Combine the layers to create a neural network\n",
    "    neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "    print(\"Stage 1) Random starting synaptic weights: \") \n",
    "    neural_network.print_weights()\n",
    "\n",
    "  # The training set. We have 7 examples, each consisting of 3 input values\n",
    "  # and 1 output value.\n",
    "    training_set_inputs = array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "    training_set_outputs = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "\n",
    "  # Train the neural network using the training set.\n",
    "  # Do it 60,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 60000)\n",
    "\n",
    "    print(\"Stage 2) New synaptic weights after training: \") \n",
    "    neural_network.print_weights()\n",
    "\n",
    "  # Test the neural network with a new situation.\n",
    "    print(\"Stage 3) Considering a new situation [1, 1, 0] -> ?: \") \n",
    "    hidden_state, output = neural_network.think(array([[1, 1, 0], [0, 1, 1]]))\n",
    "    print(output.T) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
